{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMZK0tPzHJ8CVncKyRk9NR5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OwfOITNs99YW"},"outputs":[],"source":["# !pip install mltu\n","import os\n","from datetime import datetime\n","from mltu.configs import BaseModelConfigs\n","class ModelConfigs(BaseModelConfigs):\n","    def __init__(self):\n","        super().__init__()\n","        self.model_path = os.path.join(\"Models/05_sound_to_text\", datetime.strftime(datetime.now(), \"%Y%m%d\"))\n","        self.frame_length = 256\n","        self.frame_step = 160\n","        self.fft_length = 384\n","\n","        self.vocab = \"abcdefghijklmnopqrstuvwxyz'?! \"\n","        self.input_shape = None\n","        self.max_text_length = None\n","        self.max_spectrogram_length = None\n","\n","        self.batch_size = 8\n","        self.learning_rate = 0.0005\n","        ############################\n","        self.train_epochs = 50\n","        ############################\n","        self.train_workers = 20"]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras import layers\n","from keras.models import Model\n","from mltu.tensorflow.model_utils import residual_block, activation_layer\n","def train_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n","    inputs = layers.Input(shape=input_dim, name=\"input\", dtype=tf.float32)\n","    # expand dims to add channel dimension\n","    input = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n","    # Convolution layer 1\n","    x = layers.Conv2D(filters=32, kernel_size=[11, 41], strides=[2, 2], padding=\"same\", use_bias=False)(input)\n","    x = layers.BatchNormalization()(x)\n","    x = activation_layer(x, activation=\"leaky_relu\")\n","    # Convolution layer 2\n","    x = layers.Conv2D(filters=32, kernel_size=[11, 21], strides=[1, 2], padding=\"same\", use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = activation_layer(x, activation=\"leaky_relu\")\n","    # Reshape the resulted volume to feed the RNNs layers\n","    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n","    # RNN layers\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","    x = layers.Dropout(dropout)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","    x = layers.Dropout(dropout)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","    x = layers.Dropout(dropout)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","    x = layers.Dropout(dropout)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","\n","    # Dense layer\n","    x = layers.Dense(256)(x)\n","    x = activation_layer(x, activation=\"leaky_relu\")\n","    x = layers.Dropout(dropout)(x)\n","    # Classification layer\n","    output = layers.Dense(output_dim + 1, activation=\"softmax\", dtype=tf.float32)(x)\n","    model = Model(inputs=inputs, outputs=output)\n","    return model"],"metadata":{"id":"_zexHr6S-bMs","executionInfo":{"status":"ok","timestamp":1709716046123,"user_tz":-330,"elapsed":3400,"user":{"displayName":"Andrew Jerry","userId":"17543747561368286358"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# !pip install tf2onnx\n","import tensorflow as tf\n","try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n","except: pass\n","\n","import os\n","import csv\n","import tarfile\n","import pandas as pd\n","from tqdm import tqdm\n","from urllib.request import urlopen\n","from io import BytesIO\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","from mltu.preprocessors import WavReader\n","\n","from mltu.tensorflow.dataProvider import DataProvider\n","from mltu.transformers import LabelIndexer, LabelPadding, SpectrogramPadding\n","from mltu.tensorflow.losses import CTCloss\n","from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n","from mltu.tensorflow.metrics import CERMetric, WERMetric\n","\n","\n","def download_and_unzip(url, extract_to=\"Datasets\", chunk_size=1024*1024):\n","    http_response = urlopen(url)\n","\n","    data = b\"\"\n","    iterations = http_response.length // chunk_size + 1\n","    for _ in tqdm(range(iterations)):\n","        data += http_response.read(chunk_size)\n","\n","    tarFile = tarfile.open(fileobj=BytesIO(data))\n","    tarFile.extractall(path=extract_to)\n","    tarFile.close()\n","\n","\n","dataset_path = os.path.join(\"Datasets\", \"nptel-pure\")\n","if not os.path.exists(dataset_path):\n","    download_and_unzip(\"https://github.com/AI4Bharat/NPTEL2020-Indian-English-Speech-Dataset/releases/download/v0.1/nptel-pure-set.tar.gz\", extract_to=\"Datasets\")\n","\n","dataset_path = \"Datasets/nptel-pure\"\n","metadata_path = dataset_path + \"/metadata.csv\"\n","wavs_path = dataset_path + \"/wav/\"\n","\n","# Read metadata file and parse it\n","files = os.listdir('Datasets/nptel-pure/original_txt')\n","with open('Datasets/nptel-pure/metadata.csv', 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","    writer.writerow(['file_name', 'normalized_transcription'])\n","    for file in files:\n","        with open(os.path.join('Datasets/nptel-pure/original_txt', file), 'r') as txt_file:\n","            text = txt_file.read()\n","        writer.writerow([file.replace(\".txt\",\"\"), text])\n","metadata_df = pd.read_csv(metadata_path, sep=\",\", header=0, quoting=3)\n","\n","# structure the dataset where each row is a list of [wav_file_path, sound transcription]\n","dataset = [[f\"Datasets/nptel-pure/wav/{file}.wav\", label.lower()] for file, label in metadata_df.values.tolist()]\n","# Create a ModelConfigs object to store model configurations\n","configs = ModelConfigs()\n","\n","max_text_length, max_spectrogram_length = 0, 0\n","for file_path, label in tqdm(dataset):\n","    spectrogram = WavReader.get_spectrogram(file_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n","    valid_label = [c for c in label if c in configs.vocab]\n","    max_text_length = max(max_text_length, len(valid_label))\n","    max_spectrogram_length = max(max_spectrogram_length, spectrogram.shape[0])\n","    configs.input_shape = [max_spectrogram_length, spectrogram.shape[1]]\n","\n","configs.max_spectrogram_length = max_spectrogram_length\n","configs.max_text_length = max_text_length\n","configs.save()\n","# Create a data provider for the dataset\n","data_provider = DataProvider(\n","    dataset=dataset,\n","    skip_validation=True,\n","    batch_size=configs.batch_size,\n","    data_preprocessors=[\n","        WavReader(frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length),\n","        ],\n","    transformers=[\n","        SpectrogramPadding(max_spectrogram_length=configs.max_spectrogram_length, padding_value=0),\n","        LabelIndexer(configs.vocab),\n","        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n","        ],\n",")\n","\n","# Split the dataset into training and validation sets\n","train_data_provider, val_data_provider = data_provider.split(split = 0.9)\n","# Creating TensorFlow model architecture\n","model = train_model(\n","    input_dim = configs.input_shape,\n","    output_dim = len(configs.vocab),\n","    dropout=0.5\n",")\n","# Compile the model and print summary\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n","    loss=CTCloss(),\n","    metrics=[\n","        CERMetric(vocabulary=configs.vocab),\n","        WERMetric(vocabulary=configs.vocab)\n","        ],\n","    run_eagerly=False\n",")\n","model.summary(line_length=110)\n","# Define callbacks\n","earlystopper = EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\")\n","checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n","trainLogger = TrainLogger(configs.model_path)\n","tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n","reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.8, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n","model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\")\n","# Train the model\n","model.fit(\n","    train_data_provider,\n","    validation_data=val_data_provider,\n","    epochs=configs.train_epochs,\n","    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx],\n","    workers=configs.train_workers\n",")\n","# Save training and validation datasets as csv files\n","train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n","val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9DcRRAT0-eTu","executionInfo":{"status":"ok","timestamp":1709719167075,"user_tz":-330,"elapsed":3062715,"user":{"displayName":"Andrew Jerry","userId":"17543747561368286358"}},"outputId":"952d8312-3022-4e97-9473-6b8474391d90"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n","Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.25.2)\n","Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (23.5.26)\n","Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.2.2)\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1000 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["librosa version: 0.10.1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:13<00:00, 73.39it/s]\n","INFO:DataProvider:Skipping Dataset validation...\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","______________________________________________________________________________________________________________\n"," Layer (type)                                    Output Shape                                Param #          \n","==============================================================================================================\n"," input (InputLayer)                              [(None, 2720, 193)]                         0                \n","                                                                                                              \n"," lambda (Lambda)                                 (None, 2720, 193, 1)                        0                \n","                                                                                                              \n"," conv2d (Conv2D)                                 (None, 1360, 97, 32)                        14432            \n","                                                                                                              \n"," batch_normalization (BatchNormalization)        (None, 1360, 97, 32)                        128              \n","                                                                                                              \n"," leaky_re_lu (LeakyReLU)                         (None, 1360, 97, 32)                        0                \n","                                                                                                              \n"," conv2d_1 (Conv2D)                               (None, 1360, 49, 32)                        236544           \n","                                                                                                              \n"," batch_normalization_1 (BatchNormalization)      (None, 1360, 49, 32)                        128              \n","                                                                                                              \n"," leaky_re_lu_1 (LeakyReLU)                       (None, 1360, 49, 32)                        0                \n","                                                                                                              \n"," reshape (Reshape)                               (None, 1360, 1568)                          0                \n","                                                                                                              \n"," bidirectional (Bidirectional)                   (None, 1360, 256)                           1737728          \n","                                                                                                              \n"," dropout (Dropout)                               (None, 1360, 256)                           0                \n","                                                                                                              \n"," bidirectional_1 (Bidirectional)                 (None, 1360, 256)                           394240           \n","                                                                                                              \n"," dropout_1 (Dropout)                             (None, 1360, 256)                           0                \n","                                                                                                              \n"," bidirectional_2 (Bidirectional)                 (None, 1360, 256)                           394240           \n","                                                                                                              \n"," dropout_2 (Dropout)                             (None, 1360, 256)                           0                \n","                                                                                                              \n"," bidirectional_3 (Bidirectional)                 (None, 1360, 256)                           394240           \n","                                                                                                              \n"," dropout_3 (Dropout)                             (None, 1360, 256)                           0                \n","                                                                                                              \n"," bidirectional_4 (Bidirectional)                 (None, 1360, 256)                           394240           \n","                                                                                                              \n"," dense (Dense)                                   (None, 1360, 256)                           65792            \n","                                                                                                              \n"," leaky_re_lu_2 (LeakyReLU)                       (None, 1360, 256)                           0                \n","                                                                                                              \n"," dropout_4 (Dropout)                             (None, 1360, 256)                           0                \n","                                                                                                              \n"," dense_1 (Dense)                                 (None, 1360, 31)                            7967             \n","                                                                                                              \n","==============================================================================================================\n","Total params: 3639679 (13.88 MB)\n","Trainable params: 3639551 (13.88 MB)\n","Non-trainable params: 128 (512.00 Byte)\n","______________________________________________________________________________________________________________\n","Epoch 1/50\n","113/113 [==============================] - ETA: 0s - loss: 626.4393 - CER: 1.1424 - WER: 0.9993\n","Epoch 1: val_CER improved from inf to 0.98817, saving model to Models/05_sound_to_text/20240306/model.h5\n","113/113 [==============================] - 158s 1s/step - loss: 626.4393 - CER: 1.1424 - WER: 0.9993 - val_loss: 504.2588 - val_CER: 0.9882 - val_WER: 0.9991 - lr: 5.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50\n","113/113 [==============================] - ETA: 0s - loss: 476.9135 - CER: 1.0025 - WER: 1.0348\n","Epoch 2: val_CER improved from 0.98817 to 0.98817, saving model to Models/05_sound_to_text/20240306/model.h5\n","113/113 [==============================] - 114s 1s/step - loss: 476.9135 - CER: 1.0025 - WER: 1.0348 - val_loss: 436.7296 - val_CER: 0.9882 - val_WER: 0.9991 - lr: 5.0000e-04\n","Epoch 3/50\n","113/113 [==============================] - ETA: 0s - loss: 401.8163 - CER: 0.9479 - WER: 1.0370\n","Epoch 3: val_CER improved from 0.98817 to 0.98165, saving model to Models/05_sound_to_text/20240306/model.h5\n","113/113 [==============================] - 117s 1s/step - loss: 401.8163 - CER: 0.9479 - WER: 1.0370 - val_loss: 370.6367 - val_CER: 0.9816 - val_WER: 0.9991 - lr: 5.0000e-04\n","Epoch 4/50\n","113/113 [==============================] - ETA: 0s - loss: 319.5566 - CER: 0.9096 - WER: 1.0916\n","Epoch 4: val_CER did not improve from 0.98165\n","113/113 [==============================] - 115s 1s/step - loss: 319.5566 - CER: 0.9096 - WER: 1.0916 - val_loss: 285.9801 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 5.0000e-04\n","Epoch 5/50\n","113/113 [==============================] - ETA: 0s - loss: 263.1064 - CER: 0.9996 - WER: 1.0000\n","Epoch 5: val_CER did not improve from 0.98165\n","113/113 [==============================] - 116s 1s/step - loss: 263.1064 - CER: 0.9996 - WER: 1.0000 - val_loss: 281.2605 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 5.0000e-04\n","Epoch 6/50\n","113/113 [==============================] - ETA: 0s - loss: 263.2389 - CER: 0.9998 - WER: 1.0000\n","Epoch 6: val_CER did not improve from 0.98165\n","113/113 [==============================] - 115s 996ms/step - loss: 263.2389 - CER: 0.9998 - WER: 1.0000 - val_loss: 289.1818 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 5.0000e-04\n","Epoch 7/50\n","113/113 [==============================] - ETA: 0s - loss: 262.7067 - CER: 0.9998 - WER: 0.9999\n","Epoch 7: val_CER did not improve from 0.98165\n","113/113 [==============================] - 114s 1s/step - loss: 262.7067 - CER: 0.9998 - WER: 0.9999 - val_loss: 284.0001 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 5.0000e-04\n","Epoch 8/50\n","113/113 [==============================] - ETA: 0s - loss: 262.6933 - CER: 0.9997 - WER: 1.0000\n","Epoch 8: val_CER did not improve from 0.98165\n","\n","Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n","113/113 [==============================] - 116s 1s/step - loss: 262.6933 - CER: 0.9997 - WER: 1.0000 - val_loss: 280.1191 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 5.0000e-04\n","Epoch 9/50\n","113/113 [==============================] - ETA: 0s - loss: 262.5668 - CER: 0.9998 - WER: 1.0000\n","Epoch 9: val_CER did not improve from 0.98165\n","113/113 [==============================] - 113s 993ms/step - loss: 262.5668 - CER: 0.9998 - WER: 1.0000 - val_loss: 282.5838 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 4.0000e-04\n","Epoch 10/50\n","113/113 [==============================] - ETA: 0s - loss: 261.8317 - CER: 0.9998 - WER: 0.9999\n","Epoch 10: val_CER did not improve from 0.98165\n","113/113 [==============================] - 113s 993ms/step - loss: 261.8317 - CER: 0.9998 - WER: 0.9999 - val_loss: 279.5684 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 4.0000e-04\n","Epoch 11/50\n","113/113 [==============================] - ETA: 0s - loss: 261.7397 - CER: 0.9996 - WER: 1.0000\n","Epoch 11: val_CER did not improve from 0.98165\n","113/113 [==============================] - 113s 994ms/step - loss: 261.7397 - CER: 0.9996 - WER: 1.0000 - val_loss: 271.9688 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 4.0000e-04\n","Epoch 12/50\n","113/113 [==============================] - ETA: 0s - loss: 261.3360 - CER: 0.9997 - WER: 0.9999\n","Epoch 12: val_CER did not improve from 0.98165\n","113/113 [==============================] - 115s 1s/step - loss: 261.3360 - CER: 0.9997 - WER: 0.9999 - val_loss: 272.4571 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 4.0000e-04\n","Epoch 13/50\n","113/113 [==============================] - ETA: 0s - loss: 261.0307 - CER: 0.9999 - WER: 1.0000\n","Epoch 13: val_CER did not improve from 0.98165\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n","113/113 [==============================] - 113s 990ms/step - loss: 261.0307 - CER: 0.9999 - WER: 1.0000 - val_loss: 268.7678 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 4.0000e-04\n","Epoch 14/50\n","113/113 [==============================] - ETA: 0s - loss: 260.7908 - CER: 0.9998 - WER: 1.0000\n","Epoch 14: val_CER did not improve from 0.98165\n","113/113 [==============================] - 115s 1s/step - loss: 260.7908 - CER: 0.9998 - WER: 1.0000 - val_loss: 267.1791 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 3.2000e-04\n","Epoch 15/50\n","113/113 [==============================] - ETA: 0s - loss: 260.7037 - CER: 0.9996 - WER: 1.0000\n","Epoch 15: val_CER did not improve from 0.98165\n","113/113 [==============================] - 116s 1s/step - loss: 260.7037 - CER: 0.9996 - WER: 1.0000 - val_loss: 269.8493 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 3.2000e-04\n","Epoch 16/50\n","113/113 [==============================] - ETA: 0s - loss: 260.5764 - CER: 0.9996 - WER: 0.9998\n","Epoch 16: val_CER did not improve from 0.98165\n","113/113 [==============================] - 117s 1s/step - loss: 260.5764 - CER: 0.9996 - WER: 0.9998 - val_loss: 266.5842 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 3.2000e-04\n","Epoch 17/50\n","113/113 [==============================] - ETA: 0s - loss: 260.4981 - CER: 0.9998 - WER: 1.0000\n","Epoch 17: val_CER did not improve from 0.98165\n","113/113 [==============================] - 116s 1s/step - loss: 260.4981 - CER: 0.9998 - WER: 1.0000 - val_loss: 273.4341 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 3.2000e-04\n","Epoch 18/50\n","113/113 [==============================] - ETA: 0s - loss: 260.3212 - CER: 0.9996 - WER: 0.9999\n","Epoch 18: val_CER did not improve from 0.98165\n","\n","Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002560000168159604.\n","113/113 [==============================] - 117s 1s/step - loss: 260.3212 - CER: 0.9996 - WER: 0.9999 - val_loss: 267.9885 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 3.2000e-04\n","Epoch 19/50\n","113/113 [==============================] - ETA: 0s - loss: 260.2820 - CER: 0.9997 - WER: 1.0000\n","Epoch 19: val_CER did not improve from 0.98165\n","113/113 [==============================] - 117s 1s/step - loss: 260.2820 - CER: 0.9997 - WER: 1.0000 - val_loss: 278.1869 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 2.5600e-04\n","Epoch 20/50\n","113/113 [==============================] - ETA: 0s - loss: 260.5561 - CER: 0.9997 - WER: 0.9999\n","Epoch 20: val_CER did not improve from 0.98165\n","113/113 [==============================] - 116s 1s/step - loss: 260.5561 - CER: 0.9997 - WER: 0.9999 - val_loss: 266.2326 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 2.5600e-04\n","Epoch 21/50\n","113/113 [==============================] - ETA: 0s - loss: 260.1858 - CER: 0.9997 - WER: 1.0000\n","Epoch 21: val_CER did not improve from 0.98165\n","113/113 [==============================] - 117s 1s/step - loss: 260.1858 - CER: 0.9997 - WER: 1.0000 - val_loss: 266.3201 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 2.5600e-04\n","Epoch 22/50\n","113/113 [==============================] - ETA: 0s - loss: 260.1955 - CER: 0.9997 - WER: 1.0000\n","Epoch 22: val_CER did not improve from 0.98165\n","113/113 [==============================] - 114s 1s/step - loss: 260.1955 - CER: 0.9997 - WER: 1.0000 - val_loss: 262.5334 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 2.5600e-04\n","Epoch 23/50\n","113/113 [==============================] - ETA: 0s - loss: 260.3985 - CER: 0.9997 - WER: 1.0000\n","Epoch 23: val_CER did not improve from 0.98165\n","\n","Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00020480002276599408.\n","113/113 [==============================] - 114s 1s/step - loss: 260.3985 - CER: 0.9997 - WER: 1.0000 - val_loss: 266.0245 - val_CER: 1.0000 - val_WER: 1.0000 - lr: 2.5600e-04\n","Epoch 23: early stopping\n"]}]},{"cell_type":"code","source":["import typing\n","import numpy as np\n","from mltu.inferenceModel import OnnxInferenceModel\n","from mltu.preprocessors import WavReader\n","from mltu.utils.text_utils import ctc_decoder, get_cer, get_wer\n","\n","class WavToTextModel(OnnxInferenceModel):\n","    def __init__(self, char_list: typing.Union[str, list], *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.char_list = char_list\n","    def predict(self, data: np.ndarray):\n","        data_pred = np.expand_dims(data, axis=0)\n","        preds = self.model.run(self.output_names, {self.input_names[0]: data_pred})[0]\n","        text = ctc_decoder(preds, self.char_list)[0]\n","        return text\n","\n","if __name__ == \"__main__\":\n","    import pandas as pd\n","    from tqdm import tqdm\n","    from mltu.configs import BaseModelConfigs\n","    configs = BaseModelConfigs.load(\"Models/05_sound_to_text/\"+datetime.strftime(datetime.now(), \"%Y%m%d\")+\"/configs.yaml\")\n","    model = WavToTextModel(model_path=configs.model_path, char_list=configs.vocab, force_cpu=True)\n","    df = pd.read_csv(\"Models/05_sound_to_text/\"+datetime.strftime(datetime.now(), \"%Y%m%d\")+\"/val.csv\").values.tolist()\n","\n","    accum_cer, accum_wer = [], []\n","    for wav_path, label in tqdm(df):\n","        wav_path = wav_path.replace(\"\\\\\", \"/\")\n","        spectrogram = WavReader.get_spectrogram(wav_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n","        WavReader.plot_raw_audio(wav_path, label)\n","        padded_spectrogram = np.pad(spectrogram, ((0, configs.max_spectrogram_length - spectrogram.shape[0]),(0,0)), mode=\"constant\", constant_values=0)\n","        WavReader.plot_spectrogram(spectrogram, label)\n","        text = model.predict(padded_spectrogram)\n","        true_label = \"\".join([l for l in label.lower() if l in configs.vocab])\n","\n","        cer = get_cer(text, true_label)\n","        wer = get_wer(text, true_label)\n","\n","        accum_cer.append(cer)\n","        accum_wer.append(wer)\n","\n","    print(f\"Average CER: {np.average(accum_cer)}, Average WER: {np.average(accum_wer)}\")"],"metadata":{"id":"kpOoJJV0QoH6"},"execution_count":null,"outputs":[]}]}